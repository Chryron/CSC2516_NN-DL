{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chryron/CSC2516_NN-DL/blob/main/CSC2516_Homework_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2hPDx3Tvh0v"
      },
      "source": [
        "# Homework 6\n",
        "\n",
        "In this homework you will be training and using a \"char-RNN\". This is the name given to a character-level recurrent neural network language model by [this famous blog post by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Before you start on the rest of the homework, please give the blog post a read, it's quite good!\n",
        "\n",
        "I don't expect you to implement the char-RNN from scratch. Andrej's original char-rnn is in Torch (the predecessor to PyTorch that is not commonly used anymore). Fortunately, there are many other implementations of this model available; for example, there is one (in both mxnet and pytorch) in chapters 8 and 9 of [the textbook](http://d2l.ai), and another pytorch one [here](https://github.com/spro/char-rnn.pytorch). **Please use one of these example implementations (or another one that you find) when completing this homework**.\n",
        "\n",
        "For this homework, please complete the following steps:\n",
        "\n",
        "1. Download and tokenize the [Shakespeare dataset](http://www.gutenberg.org/files/100/100-0.txt) at a character level. I recommend basing your solution on the following code:\n",
        "```Python\n",
        "# Remove non-alphabetical characters, lowercase, and replace whitespace with ' '\n",
        "raw_dataset = ' '.join(re.sub('[^A-Za-z ]+', '', text).lower().split())\n",
        "# Maps token index to character\n",
        "idx_to_char = list(set(raw_dataset))\n",
        "# Maps character to token index\n",
        "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
        "# Tokenize the dataset\n",
        "corpus_indices = [char_to_idx[char] for char in raw_dataset]\n",
        "```\n",
        "1. Train a \"vanilla\" RNN (as described in chapter 9 of [the textbook](http://d2l.ai)) on the Shakespeare dataset. Report the training loss and generate some samples from the model at the end of training.\n",
        "1. Train a GRU RNN (as described in chapter 10 of [the textbook](http://d2l.ai)) on the Shakespeare datatset. Is the final training loss higher or lower than the vanilla RNN? Are the samples from the model more or less realistic?\n",
        "1. Find a smaller, simpler dataset than the Shakespeare data (you can find some ideas in Andrej's blog post, but feel free to get creative!) and train either the vanilla or GRU RNN on it instead. Is the final training loss higher or lower than it was for the Shakespeare data?"
      ]
    }
  ]
}