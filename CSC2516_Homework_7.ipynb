{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chryron/CSC2516_NN-DL/blob/main/CSC2516_Homework_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovi3rNd9BeZ4"
      },
      "source": [
        "# Homework 7\n",
        "\n",
        "In this homework, you will be using a form of attention called *attention pooling* to solve the \"addition problem\". The addition problem was introduced in the [LSTM paper](https://www.bioinf.jku.at/publications/older/2604.pdf) as a way to test whether an RNN could propagate information across many time steps. In the addition problem, the model is given a sequence of 2D vectors in the format:\n",
        "\n",
        "|     |      |     |     |      |     |      |     |     |     |     |\n",
        "|-----|------|-----|-----|------|-----|------|-----|-----|-----|-----|\n",
        "| 0.5 | -0.7 | 0.3 | 0.1 | -0.2 | ... | -0.5 | 0.9 | ... | 0.8 | 0.2 |\n",
        "| 0   |   0  |  1  |  0  |   0  |     |   0  |  1  |     |  0  |  0  |\n",
        "\n",
        "The first dimension of each vector in the sequence is a random number between 0 and 1. The second dimension is 0 for all entries of the sequence except for 2 of the entries, where it is 1. The goal of the addition problem is to output the sum of the values in the first dimension at the two indices where the second dimension is 1. In the example above, the target would be 0.9 + 0.3 = 1.2. Below is a code snippet that generates a sequence and its target for the addition problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r42nn-jOxhKp"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def addition_problem(sequence_length=50):\n",
        "    output = np.random.uniform(-1, 1, (sequence_length, 2))\n",
        "    output[:, 0] = 0.\n",
        "    random_indices = np.random.choice(sequence_length, size=2, replace=False)\n",
        "    output[random_indices, [0, 0]] = 1\n",
        "    return output, (output[:, 0]*output[:, 1]).sum(keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew0FypwYCwpn"
      },
      "source": [
        "Attention pooling is a form of attention that allows a model to solve the addition problem without using an RNN. In attention pooling, the query vector $q$ is a *learnable parameter*. The keys and values are both the input sequence. Specifically, given a sequence $\\{h_1, h_2, \\ldots, h_T\\}$, attention pooling computes\n",
        "\\begin{align}\n",
        "e_t &= \\mathrm{a}(q, h_t) \\\\\n",
        "\\alpha_t &= \\frac{\\exp(e_t)}{\\sum_k \\exp(e_k)} \\\\\n",
        "c &= \\sum_{t = 1}^T \\alpha_t h_t\n",
        "\\end{align}\n",
        "where $\\mathrm{a}(q, h_t)$ is the attention energy function. Note that c will always be a fixed-length vector (which amounts to a weighted average of the elements of the sequence $h$) regardless of how long the sequence is (i.e. the value of $T$). $\\mathrm{a}(q, h_t)$ can be any function that takes in a single entry of the sequence $h_t$ and outputs an unnormalizes scalar value. One option is to use\n",
        "$$\\mathrm{a}(q, h_t) = q^\\top \\tanh(W_a h_t + b_a)$$\n",
        "where $q \\in \\mathbb{R}^q$, $W_a \\in \\mathbb{R}^{q \\times d}$, and $b_a \\in \\mathbb{R}^q$ are learnable parameters, and $d$ is the dimensionality of $h_t$ (i.e. $h_t \\in \\mathbb{R}^d$).\n",
        "\n",
        "\n",
        "1. Build and train a neural network that uses attention pooling to solve the addition problem. The model should output a scalar which corresponds to the target value for the addition problem (i.e. the sum of the sequence entries that are marked with a \"1\"). Here, \"solved\" means that the squared error of the model's predicitons is always below $0.05$. Use a sequence length of $50$ (which is the default for the `addition_problem` function defined above). *Hints*:\n",
        "  1. This is a regression problem. Your model should predict a continuous scalar value and you can use a squared-error loss.\n",
        "  1. The point of the attention pooling layer is to allow you to put it in an otherwise feed-forward network. So, consider just using simple dense feed-forward layers before and/or after the attention pooling layer. To start, you can try the architecture: feed-forward, attention pooling, feed-forward, output layer.\n",
        "  1. If you are finding that the model is getting stuck at a non-zero squared error, it could be that it's just outputting the mean value and having trouble learning a good solution. Try different initialization, nonlinearities, architecture, learning rate, etc.\n",
        "1. Once you have trained a model that gets solid performance at sequence length $50$, plot the model's average squared error for sequence lengths $50, 55, 65, 80, 100, 125, 150$. You should generate this plot by averaging the squared error over at least $100$ sequences of a given length. Does the model's error get worse (go up) for longer sequences, or does it generalize to longer sequence lengths?"
      ]
    }
  ]
}